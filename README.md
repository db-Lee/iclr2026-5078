### Train Datasets

- [train](https://huggingface.co/datasets/iclr2026-5078/train): the multi-domain training dataset for dORM/dPRM.
- [train_orm](https://huggingface.co/datasets/iclr2026-5078/train_orm): the multi-domain training dataset for gORM.
- [train_prm](https://huggingface.co/datasets/iclr2026-5078/train_prm): the multi-domain training dataset for gPRM.

---

### Test Datasets

- [test](https://huggingface.co/datasets/iclr2026-5078/test): the multi-domain test dataset generated by [Llama3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).
- [test_smollm](https://huggingface.co/datasets/iclr2026-5078/test_smollm): the multi-domain test dataset generated by [SmolLM3-3B](hf.co/HuggingFaceTB/SmolLM3-3B).
- [test_qwen](https://huggingface.co/datasets/iclr2026-5078/test_qwen): the multi-domain test dataset generated by [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct).
- [test_gemma](https://huggingface.co/datasets/iclr2026-5078/test_gemma): the multi-domain test dataset generated by [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it).
- [test_llama70B](https://huggingface.co/datasets/iclr2026-5078/test_llama70B): the multi-domain test dataset generated by [Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct).

---

### Model Checkpoints

- [dORM-14B](https://huggingface.co/datasets/iclr2026-5078/dORM-14B): dORM with [14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) backbone trained on the multi-domain training dataset ([train](https://huggingface.co/datasets/iclr2026-5078/train)).
- [dPRM-14B](https://huggingface.co/datasets/iclr2026-5078/d{RM-14B): dPRM with [14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) backbone trained on the multi-domain training dataset ([train](https://huggingface.co/datasets/iclr2026-5078/train)).
- [gORM-14B](https://huggingface.co/datasets/iclr2026-5078/gORM-14B): gORM with [14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) backbone trained on the multi-domain training dataset ([train_orm](https://huggingface.co/datasets/iclr2026-5078/train_orm)).
- [gPRM-14B](https://huggingface.co/datasets/iclr2026-5078/gPRM-14B): gPRM with [14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) backbone trained on the multi-domain training dataset ([train_prm](https://huggingface.co/datasets/iclr2026-5078/train_prm)).

---

### Code

- `versaprm/`: training/evaluation code for dicriminative variants, i.e., dORM/dPRM (mostly adapted from [VersaPRM](https://github.com/UW-Madison-Lee-Lab/VersaPRM)).
- `multigenprm/`: training/evaluation code for generative variants, i.e., gORM/gPRM.
